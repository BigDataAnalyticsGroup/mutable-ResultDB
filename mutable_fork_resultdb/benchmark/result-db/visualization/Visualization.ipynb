{
 "cells": [
  {
   "cell_type": "code",
   "id": "5f9442ef-9de6-4173-bb6c-2b5b6b3ecea2",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from natsort import natsort_keygen, natsorted # to naturally sort string columns\n",
    "\n",
    "plt.style.use('matplotlibrc')\n",
    "\n",
    "KB = 1024\n",
    "MB = 1024 * 1024\n",
    "GB = 1024 * 1024 * 1024"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e42d797e-6378-46c6-9f62-a6f4e823bace",
   "metadata": {},
   "source": [
    "def set_size(fraction_width=0.95, fraction_height=0.25):\n",
    "    width_pt = 241.14749 # column width in pt\n",
    "    height_pt = 626.0 # page height in pt\n",
    "\n",
    "    fig_width_pt = width_pt * fraction_width\n",
    "    fig_height_pt = height_pt * fraction_height\n",
    "    inches_per_pt = 1 / 72.27\n",
    "\n",
    "    fig_width_in = fig_width_pt * inches_per_pt\n",
    "    fig_height_in = fig_height_pt * inches_per_pt\n",
    "\n",
    "    return (fig_width_in, fig_height_in)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e10d0d46-0cbc-4bd8-bc33-d0e678318289",
   "metadata": {},
   "source": [
    "# define color to use throughout the notebook\n",
    "paired = matplotlib.colormaps['Paired']\n",
    "tab20 = matplotlib.colormaps['tab20']\n",
    "dark2 = matplotlib.colormaps['Dark2']\n",
    "tab20c = matplotlib.colormaps['tab20c']\n",
    "\n",
    "c_st =  tab20c(0)\n",
    "c_st_tt = tab20c(1)\n",
    "c_decomp = tab20c(3)\n",
    "\n",
    "c_rdb = tab20c(5)\n",
    "c_rdb_tt = tab20c(6)\n",
    "c_rdb_pj = tab20c(7)\n",
    "\n",
    "c_rdb_w_pj = tab20(2)\n",
    "c_rdb_wo_pj = tab20(4)\n",
    "c_redundancy = tab20(14)\n",
    "\n",
    "c_rm1 = dark2(0)\n",
    "c_rm2 = dark2(2)\n",
    "c_rm3 = dark2(5)\n",
    "c_rm4 = dark2(3)\n",
    "\n",
    "colormap = {\n",
    "    'Single Table': c_st,\n",
    "    'ST transfer time': c_st_tt,\n",
    "    'Decompose': c_decomp,\n",
    "    'RDB w/ post-join info': c_rdb_w_pj,\n",
    "    'Result DB': c_rdb,\n",
    "    'RDB transfer time': c_rdb_tt,\n",
    "    'RDB post-join': c_rdb_pj,\n",
    "    'RDB w/o post-join info': c_rdb_wo_pj,\n",
    "    '0. Single Table': c_st,\n",
    "    'RM1. Dynamic SELECT DISTINCT': c_rm1,\n",
    "    'RM2. Materialized SELECT DISTINCT': c_rm2,\n",
    "    'RM3. Dynamic Subquery': c_rm3,\n",
    "    'RM4. Materialized Subquery': c_rm4,\n",
    "    'Redundancy': c_redundancy\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# JOB queries\n",
    "job_postgres_queries = [\n",
    "    \"q1b\",\n",
    "    \"q2a\",\n",
    "    \"q3c\",\n",
    "    \"q4a\",\n",
    "    \"q5c\",\n",
    "    \"q6a\",\n",
    "    \"q7a\",\n",
    "    \"q8a\",\n",
    "    \"q9c\",\n",
    "    \"q10c\",\n",
    "    \"q11c\",\n",
    "    \"q12a\",\n",
    "    \"q13b\",\n",
    "    \"q14a\",\n",
    "    \"q15d\",\n",
    "    \"q16b\",\n",
    "    \"q17a\",\n",
    "    \"q18c\",\n",
    "    \"q19a\",\n",
    "    \"q20b\",\n",
    "    \"q21a\",\n",
    "    \"q22c\",\n",
    "    \"q23a\",\n",
    "    \"q24a\",\n",
    "    \"q25b\",\n",
    "    \"q26a\",\n",
    "    \"q27a\",\n",
    "    \"q28c\",\n",
    "    \"q29a\",\n",
    "    \"q30c\",\n",
    "    \"q31a\",\n",
    "    \"q32a\",\n",
    "    \"q33c\",\n",
    "]\n",
    "\n",
    "job_mutable_queries = [\n",
    "    \"q1b\",\n",
    "    \"q2a\",\n",
    "    \"q3c\",\n",
    "    \"q4a\",\n",
    "    \"q5c\",\n",
    "    \"q7a\",\n",
    "    \"q8a\",\n",
    "    \"q9c\",\n",
    "    \"q10c\",\n",
    "    \"q11c\",\n",
    "    \"q12a\",\n",
    "    \"q14a\",\n",
    "    \"q15d\",\n",
    "    \"q18c\",\n",
    "    \"q19a\",\n",
    "    \"q21a\",\n",
    "    \"q22c\",\n",
    "    \"q23a\",\n",
    "    \"q24a\",\n",
    "    \"q25b\",\n",
    "    \"q26a\",\n",
    "    \"q27a\",\n",
    "    \"q28c\",\n",
    "    \"q30c\",\n",
    "    \"q31a\",\n",
    "    \"q33c\",\n",
    "]\n",
    "\n",
    "job_subset = [\n",
    "    \"q3c\",\n",
    "    \"q4a\",\n",
    "    \"q9c\",\n",
    "    \"q11c\",\n",
    "    \"q16b\",\n",
    "    \"q18c\",\n",
    "    \"q22c\",\n",
    "    \"q25b\",\n",
    "    \"q28c\",\n",
    "    \"q33c\",\n",
    "]"
   ],
   "id": "1ef51071c00c7fed",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "26c8e0d8-b925-48a7-a3ab-c5f6db51a027",
   "metadata": {},
   "source": [
    "## Join-Order Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "id": "e43904c8-eab9-4591-81c1-32befb03cbcd",
   "metadata": {},
   "source": [
    "def compression_ratio(uncompressed, compressed):\n",
    "    \"\"\"\n",
    "    Compute the compression ratio.\n",
    "\n",
    "    Returns:\n",
    "    float: Compression ratio.\n",
    "    \"\"\"\n",
    "    # assert uncompressed != 0, \"uncompressed must not be zero\"\n",
    "    if compressed == 0:\n",
    "        return 0\n",
    "    return round(uncompressed / compressed, 2)\n",
    "\n",
    "def to_latex(single_table, rdb_w_post_join_info, rdb_wo_post_join_info, entries_per_row):\n",
    "    table_top = f\"\\\\begin{{table*}}\\n\"\n",
    "    table_top += f\"\\t\\\\small\\n\"\n",
    "    table_top += f\"\\t\\\\caption{{JOB result set sizes in KiB (compression ratio).}}\\n\"\n",
    "    table_top += f\"\\t\\\\label{{tab:job-result-set-sizes}}\\n\"\n",
    "\n",
    "    tabular = \"\"\n",
    "    queries = natsorted(single_table['query'].unique())\n",
    "    assert(natsorted(single_table['query'].unique()) == natsorted(rdb_w_post_join_info['query'].unique()) == natsorted(rdb_wo_post_join_info['query'].unique()))\n",
    "    num_queries = len(queries)\n",
    "    assert num_queries % entries_per_row == 0, \"number of queries should be a multiple of entries per row\"\n",
    "    num_tabulars = num_queries // entries_per_row\n",
    "    for t in range(num_tabulars):\n",
    "        tabular_queries = queries[t * entries_per_row:t * entries_per_row + entries_per_row]\n",
    "        tabular += f\"\\t\\\\begin{{tabular}}{{\\n\"\n",
    "        tabular += f\"\\t\\t|l!{{\\\\vrule width 1.5pt}}\\n\"\n",
    "        for e in range(entries_per_row):\n",
    "            tabular += f\"\\t\\tR{{\\\\cellwidth}}R{{\\\\cellwidthratio}}|\\n\"\n",
    "        tabular += f\"\\t\\t}}\\n\"\n",
    "        tabular += f\"\\t\\t\\\\hline\\n\"\n",
    "        tabular += f\"\\t\\t\\\\multicolumn{{1}}{{|c!{{\\\\vrule width 1.5pt}}}}{{\\\\textbf{{Method}}}}\\n\"\n",
    "        for q in tabular_queries:\n",
    "            tabular += f\"\\t\\t& \\\\multicolumn{{2}}{{c|}}{{\\\\textbf{{{q[1:]}}}}}\\n\"\n",
    "        tabular += f\"\\t\\t\\\\\\\\ \\\\noalign{{\\\\hrule height 1.5pt}}\\n\"\n",
    "        single_table_data = \"\\t\\tST\"\n",
    "        rdb_rp = \"\\t\\tRDB$_{\\\\text{RP}}$\"\n",
    "        rdb = \"\\t\\tRDB\"\n",
    "        for q in tabular_queries:\n",
    "            single_table_data  += f\" & {round(single_table[single_table['query'] == q]['size'].values[0], 2)} & \\\\hspace{{\\\\gap}}(1.0)\"\n",
    "            rdb_rp += f\" & {round(rdb_w_post_join_info[rdb_w_post_join_info['query'] == q]['size'].values[0], 2)} & \\\\hspace{{\\\\gap}}({round(rdb_w_post_join_info[rdb_w_post_join_info['query'] == q]['compression_ratio'].values[0], 1)})\"\n",
    "            rdb += f\" & {round(rdb_wo_post_join_info[rdb_wo_post_join_info['query'] == q]['size'].values[0], 2)} & \\\\hspace{{\\\\gap}}({round(rdb_wo_post_join_info[rdb_wo_post_join_info['query'] == q]['compression_ratio'].values[0], 1)})\"\n",
    "        tabular += f\"{single_table_data}\\\\\\\\\\\\hline\\n\"\n",
    "        tabular += f\"{rdb_rp}\\\\\\\\\\\\hline\\n\"\n",
    "        tabular += f\"{rdb}\\\\\\\\\\\\hline\\n\"\n",
    "        tabular += f\"\\t\\\\end{{tabular}}\\n\"\n",
    "    table_bottom = f\"\\\\end{{table*}}\"\n",
    "    latex = f\"{table_top}{tabular}{table_bottom}\"\n",
    "    print(latex)\n",
    "\n",
    "data = pd.read_csv('../result-set-sizes/job/result-set-sizes.csv')\n",
    "data = data.drop(['relation', 'count'], axis=1)\n",
    "data = data.groupby(by=['database', 'query', 'method'], as_index=False).sum()\n",
    "data = data.sort_values(by=['query', 'method'], key=natsort_keygen())\n",
    "data['size'] = round(data['size'] / KB, 2)\n",
    "\n",
    "queries = job_subset\n",
    "\n",
    "data = data[data['query'].isin(queries)]\n",
    "\n",
    "single_table = data[data['method'] == 'Single Table']\n",
    "rdb_w_post_join_info = data[data['method'] == 'rdb_w_post_join_info']\n",
    "rdb_w_post_join_info['compression_ratio'] = [ compression_ratio(baseline, new) for baseline, new in zip(single_table['size'], rdb_w_post_join_info['size']) ]\n",
    "rdb_wo_post_join_info = data[data['method'] == 'rdb_wo_post_join_info']\n",
    "rdb_wo_post_join_info['compression_ratio'] = [ compression_ratio(baseline, new) for baseline, new in zip(single_table['size'], rdb_wo_post_join_info['size']) ]\n",
    " \n",
    "# display(single_table)\n",
    "# display(rdb_wo_post_join_info)\n",
    "# display(rdb_wo_post_join_info)\n",
    "to_latex(single_table, rdb_w_post_join_info, rdb_wo_post_join_info, 5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Synthetic Star Schema (S3)",
   "id": "09da871a-b636-49ed-80a0-2e2bbcd5008c"
  },
  {
   "cell_type": "code",
   "id": "43010f28-fb1c-4bd2-a95a-816a823906d7",
   "metadata": {},
   "source": [
    "def compute_s3_result_set_sizes():\n",
    "    num_dim_tables = 4 # number of dimension tables\n",
    "    dim_payload = 20 # byte\n",
    "    dim_row_size = 4 + dim_payload # 4 byte primary key + payload\n",
    "    dim_table_size = 60\n",
    "    selectivities = np.arange(0.1, 1.1, 0.1)\n",
    "    dim_table_sizes = [ int(dim_table_size * sel) for sel in selectivities ]\n",
    "\n",
    "    fact_payload = 20\n",
    "    fact_row_size = 4 + num_dim_tables * 4 + fact_payload # 4 byte pk + 4 byte foreign key per dimension table + payload\n",
    "    fact_size = dim_table_size**num_dim_tables\n",
    "    fact_table_sizes = [ dim_size**num_dim_tables for dim_size in dim_table_sizes ]\n",
    "\n",
    "    single_table = [ ((fact_row_size + dim_row_size * num_dim_tables) * f_size) / MB for f_size in fact_table_sizes ]\n",
    "    rdb_with_post_join = ([ ((f_size * fact_row_size) + (dim_row_size * d_size * num_dim_tables)) / MB\n",
    "                           for f_size, d_size in zip(fact_table_sizes, dim_table_sizes) ])\n",
    "    rdb_without_post_join = [ (dim_payload * d_size * num_dim_tables + f_size * (fact_payload)) / MB for f_size, d_size in zip(fact_table_sizes, dim_table_sizes) ]\n",
    "\n",
    "    return single_table, rdb_with_post_join, rdb_without_post_join\n",
    "\n",
    "single_table, rdb_with_post_join, rdb_without_post_join = compute_s3_result_set_sizes()\n",
    "selectivities = np.arange(0.1, 1.1, 0.1)\n",
    "\n",
    "### Plot\n",
    "fig, ax = plt.subplots(figsize=set_size(fraction_width=1.0, fraction_height=0.2), layout='constrained')\n",
    "ax.plot(selectivities, single_table, 'o--', color=colormap['Single Table'], label='Single Table')\n",
    "ax.plot(selectivities, rdb_with_post_join, 'x--', color=colormap['RDB w/ post-join info'], label=r'RDB$_{\\text{RP}}$')\n",
    "ax.plot(selectivities, rdb_without_post_join, '^--', color=colormap['RDB w/o post-join info'], label='RDB')\n",
    "\n",
    "ax.fill_between(selectivities, single_table, rdb_with_post_join, color=colormap['Redundancy'], label='Denormalization redundancy')\n",
    "\n",
    "ax.set_xlabel('Filter selectivity on dimension tables')\n",
    "ax.set_ylabel('Result set size [MiB]')\n",
    "ax.tick_params(axis='y')\n",
    "ax.legend(loc='upper left', ncols=1)\n",
    "\n",
    "fig.savefig('synthetic-result-sizes.pdf', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "258ebb92-9d68-4c8e-9a92-7f222c29dee4",
   "metadata": {},
   "source": [
    "# Rewrite Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d023a-fae6-4a3a-9760-6b72b93469b8",
   "metadata": {},
   "source": [
    "## Join-Order Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "id": "caa4da4c-0147-460c-882d-347edcc26918",
   "metadata": {},
   "source": [
    "def preprocess_data(data):\n",
    "    # construct the following dictionary:\n",
    "    # {\n",
    "    #   'RM0': [q0_time, q1_time, ..., qn-1_time]\n",
    "    #   'RM1': [q0_time, q1_time, ..., qn-1_time]\n",
    "    # }\n",
    "    data = data.sort_values(by=['query', 'method'], key=natsort_keygen()) # ensure that data is sorted correctly\n",
    "    method_times = defaultdict(list)\n",
    "    for _, row in data.iterrows():\n",
    "        method_times[row['method']].append(row['time'])\n",
    "    return method_times\n",
    "\n",
    "def bar_plot(data, data_transfer, filename):\n",
    "    queries = natsorted(data['query'].unique())\n",
    "    queries = [ q[1:] for q in queries ]\n",
    "    method_times = preprocess_data(data[data['data_transfer'] == data_transfer])\n",
    "    x = np.arange(len(queries))  # the label locations\n",
    "    width = 0.18  # the width of the bars\n",
    "    multiplier = 0\n",
    "    fig, ax = plt.subplots(figsize=set_size(fraction_width=2, fraction_height=0.21), layout='constrained')\n",
    "    for method, times in method_times.items():\n",
    "        offset = width * multiplier\n",
    "        if method == 'RM1. Dynamic SELECT DISTINCT':\n",
    "            rects = ax.bar(x + offset, times, width, color=colormap[method], label=r'RM1. Dynamic \\texttt{SELECT DISTINCT}')      \n",
    "        elif method == 'RM2. Materialized SELECT DISTINCT':\n",
    "            rects = ax.bar(x + offset, times, width, color=colormap[method], label=r'RM2. Materialized \\texttt{SELECT DISTINCT}')\n",
    "        else:\n",
    "            rects = ax.bar(x + offset, times, width, color=colormap[method], label=method)\n",
    "        multiplier += 1\n",
    "    ax.set_xticks(x + width + width/2, queries)\n",
    "    ax.set_xlabel('JOB queries')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.xaxis.grid(False) # Disable grid lines on the x-axis\n",
    "\n",
    "    log_scale = True\n",
    "    if log_scale:\n",
    "        ax.set_ylabel('Query execution time [ms]')\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.set_ylabel('Query execution time [ms]')\n",
    "        ax.set_ylim(0)\n",
    "\n",
    "    fig.savefig(filename, bbox_inches='tight')\n",
    "\n",
    "data = pd.read_csv('../rewrite-methods/job/rewrite-results.csv')\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer', 'run'], as_index=False).sum()\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer'], as_index=False).median().drop(['run', 'num_query_internal'], axis=1)\n",
    "data = data.drop(['database', 'system'], axis=1)\n",
    "include_queries = job_postgres_queries\n",
    "data = data[data['query'].isin(include_queries)]\n",
    "exclude_methods = [\n",
    "    '0. Single Table'\n",
    "]\n",
    "data = data[~data['method'].isin(exclude_methods)]\n",
    "\n",
    "bar_plot(data, False, 'job-rewrite-methods.pdf')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf0ae1f3-1ba6-48fb-8338-e2fdef97b632",
   "metadata": {},
   "source": [
    "def overhead(baseline, new):\n",
    "    assert baseline != 0, \"baseline must not be zero\"\n",
    "    if (new <= baseline): # new is faster -> improvement\n",
    "        return -((baseline - new) / baseline) * 100\n",
    "    else: # new is slower -> overhead \n",
    "        return ((new - baseline) / baseline) * 100\n",
    "\n",
    "def to_latex(single_table, best_rewrite_method, entries_per_row):\n",
    "    table_top = f\"\\\\begin{{table*}}\\n\"\n",
    "    table_top += f\"\\t\\\\small\\n\"\n",
    "    table_top += f\"\\t\\\\caption{{Overhead of the \\\\textbf{{best}} rewrite method compared to the single-table execution time for the IMDb dataset.}} \\n\"\n",
    "    table_top += f\"\\t\\\\label{{tab:job-overhead-rewrites}}\\n\"\n",
    "    \n",
    "    tabular = \"\"\n",
    "    queries = natsorted(single_table['query'].unique())\n",
    "    assert natsorted(single_table['query'].unique()) == natsorted(best_rewrite_method['query'].unique())\n",
    "    num_queries = len(queries)\n",
    "    assert num_queries % entries_per_row == 0, \"number of queries should be a multiple of entries per row\"\n",
    "    num_tabulars = num_queries // entries_per_row\n",
    "    for t in range(num_tabulars):\n",
    "        tabular_queries = queries[t * entries_per_row:t * entries_per_row + entries_per_row]\n",
    "        tabular += f\"\\t\\\\begin{{tabular}}{{\\n\"\n",
    "        tabular += f\"\\t\\t|l!{{\\\\vrule width 1.5pt}}\\n\"\n",
    "        for e in range(entries_per_row):\n",
    "            tabular += f\"\\t\\tP{{\\\\cellwidthperfratio}}|\\n\"\n",
    "        tabular += f\"\\t\\t}}\\n\"\n",
    "        tabular += f\"\\t\\t\\\\hline\\n\"\n",
    "        tabular += f\"\\t\\t\\\\textbf{{JOB Query}}\\n\"\n",
    "        for q in tabular_queries:\n",
    "            tabular += f\"\\t\\t& \\\\textbf{{{q[1:]}}}\\n\"\n",
    "        tabular += f\"\\t\\t\\\\\\\\ \\\\noalign{{\\\\hrule height 1.5pt}}\\n\"\n",
    "        overhead_data = \"\\t\\tOverhead\"\n",
    "        best_rm_data = \"\\t\\tBest RM\"\n",
    "        for q in tabular_queries:\n",
    "            overhead_value = round(overhead(int(single_table[single_table['query'] == q]['time'].values[0]), int(best_rewrite_method[best_rewrite_method['query'] == q]['time'].values[0])), 1)\n",
    "            overhead_data  += f\" & {overhead_value}\\\\%\"\n",
    "            best_rm = best_rewrite_method[best_rewrite_method['query'] == q]['method'].values[0]\n",
    "            best_rm_data += f\" & RM~{best_rm[2]}\"\n",
    "        tabular += f\"{overhead_data}\\\\\\\\\\n\"\n",
    "        tabular += f\"{best_rm_data}\\\\\\\\\\\\hline\\n\"\n",
    "        tabular += f\"\\t\\\\end{{tabular}}\\n\"\n",
    "    table_bottom = f\"\\\\end{{table*}}\\n\"\n",
    "    latex = f\"{table_top}{tabular}{table_bottom}\"\n",
    "    print(latex)\n",
    "\n",
    "data = pd.read_csv('../rewrite-methods/job/rewrite-results.csv')\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer', 'run'], as_index=False).sum()\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer'], as_index=False).median().drop(['run', 'num_query_internal'], axis=1)\n",
    "data = data.drop(['database', 'system'], axis=1)\n",
    "data = data[data['data_transfer'] == False]\n",
    "include_queries = job_postgres_queries\n",
    "data = data[data['query'].isin(include_queries)]\n",
    "\n",
    "single_table = data[data['method'] == '0. Single Table']\n",
    "rewrite_methods = data[data['method'] != '0. Single Table']\n",
    "\n",
    "# NOTE: be careful as ties are included (handle them manually)\n",
    "min_values = rewrite_methods.groupby(by=['query'])['time'].transform('min')\n",
    "best_rewrite_method = rewrite_methods[rewrite_methods['time'] == min_values]\n",
    "\n",
    "# ensure that both dataframes are sorted correctly\n",
    "single_table = single_table.sort_values(by=['query', 'method'], key=natsort_keygen()) # ensure that data is sorted correctly\n",
    "best_rewrite_method = best_rewrite_method.sort_values(by=['query', 'method'], key=natsort_keygen()) # ensure that data is sorted correctly\n",
    "\n",
    "assert len(single_table) == len(best_rewrite_method), f\"{len(single_table)} vs {len(best_rewrite_method)}\"\n",
    "\n",
    "to_latex(single_table, best_rewrite_method, 11)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "483ff91c-8c3f-4fef-9cec-88d3a3f7de8d",
   "metadata": {},
   "source": [
    "# RESULTDB Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db905c6-1618-4ad1-8105-d7de161babb4",
   "metadata": {},
   "source": [
    "## Join-Order Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "id": "7b057539-dad4-45b3-b9e6-f24a071e213a",
   "metadata": {},
   "source": [
    "# construct the following dictionary:\n",
    "# {\n",
    "#   'Single-Table': [q4_time, q5_time, ...]\n",
    "#   'Single-Table + Decompose': [q4_time, q5_time, ...]\n",
    "#   'Result-DB': [q4_time, q5_time, ...]\n",
    "# }\n",
    "# ensure that `queries` is sorted!\n",
    "queries = job_mutable_queries\n",
    "\n",
    "single_table_decompose = []\n",
    "resultdb = []\n",
    "processed_queries = []\n",
    "num_queries = 0\n",
    "for query in queries:\n",
    "    result_file = Path(f\"../algorithm/job/{query}_results.csv\")\n",
    "    if not result_file.is_file():\n",
    "        print(f\"{result_file} does not exist. Skipping...\")\n",
    "        continue\n",
    "    processed_queries.append(query)\n",
    "    data = pd.read_csv(result_file)  \n",
    "    data = (data.groupby(by=['commit', 'date', 'version', 'suite', 'benchmark', 'experiment', 'name', 'config', 'case',], as_index=False)\n",
    "                .median()\n",
    "                .drop(['commit', 'date', 'version', 'suite', 'benchmark', 'case', 'runid'], axis=1)\n",
    "           )\n",
    "    experiment = data['experiment'].unique()\n",
    "    assert len(experiment) == 1, f'experiment contains multiple different queries'\n",
    "    assert experiment[0] == query, f'experiment query: {experiment[0]} does not match query {query} of current file'\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        algorithm = row['name']\n",
    "        execution_time = row['time']\n",
    "        if \"single-table\" in algorithm:\n",
    "            single_table_execution_time = execution_time\n",
    "        elif \"decompose\" in algorithm:\n",
    "            decompose_execution_time = execution_time\n",
    "        elif \"resultdb\" in algorithm:\n",
    "            resultdb_execution_time = execution_time\n",
    "        else:\n",
    "            assert False, \"experiment name: {name} does not match any of our algorithms\"\n",
    "\n",
    "    single_table_decompose.append((single_table_execution_time, decompose_execution_time - single_table_execution_time))\n",
    "    resultdb.append(resultdb_execution_time)\n",
    "\n",
    "assert len(single_table_decompose) == len(resultdb) == len(processed_queries), f'number of measurements has to match the processed queries'\n",
    "\n",
    "# plot data\n",
    "x = np.arange(len(processed_queries))  # the label locations\n",
    "width = 0.30  # the width of the bars\n",
    "fig, main_ax = plt.subplots(figsize=set_size(fraction_width=1, fraction_height=0.21), layout='constrained')\n",
    "\n",
    "st_time = [ st_decomp[0] for st_decomp in single_table_decompose ]\n",
    "decomp_time = [ st_decomp[1] for st_decomp in single_table_decompose ]\n",
    "# ZOOM IN\n",
    "inset_ax = main_ax.inset_axes([0.4, 0.35, 0.1, 0.2],\n",
    "                              xlim=[12.8, 13.5], ylim=[1520, 1540],\n",
    "                              xticks = [],\n",
    "                              xticklabels=[],\n",
    "                              )\n",
    "for ax in main_ax, inset_ax:\n",
    "    ax.bar(x, st_time, width, color=colormap['Single Table'], edgecolor='black', label='Single Table')\n",
    "    ax.bar(x, decomp_time, width, bottom=st_time, color=colormap['Decompose'], edgecolor='black', label='Decompose')\n",
    "    offset = width\n",
    "    ax.bar(x + offset, resultdb, width, color=colormap['Result DB'], edgecolor='black', label=r'\\textsc{ResultDB$_{\\text{semi-join}}$}')\n",
    "\n",
    "main_ax.indicate_inset_zoom(inset_ax)\n",
    "\n",
    "main_ax.set_ylabel('Query execution time [ms]')\n",
    "main_ax.set_xlabel('JOB queries')\n",
    "processed_queries = [ q[1:] for q in processed_queries ]\n",
    "main_ax.set_xticks(x + width / 2, processed_queries, rotation=90)\n",
    "main_ax.legend(loc='upper left')\n",
    "main_ax.set_ylim(0)\n",
    "main_ax.xaxis.grid(False) # Disable grid lines on the x-axis\n",
    "\n",
    "fig.savefig('job-algorithm.pdf', bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Post-join\n",
   "id": "3d890f737afaa8a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Join-Order Benchmark",
   "id": "ec95e76e86ba75ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "queries = [q for q in job_subset if q != \"q16b\"]\n",
    "\n",
    "post_join_times = []\n",
    "processed_queries = []\n",
    "num_queries = 0\n",
    "for query in queries:\n",
    "    result_file = Path(f\"../post-join/job/{query}/{query}_results.csv\")\n",
    "    if not result_file.is_file():\n",
    "        print(f\"{result_file} does not exist. Skipping...\")\n",
    "        continue\n",
    "    processed_queries.append(query)\n",
    "    data = pd.read_csv(result_file)  \n",
    "    data = (data.groupby(by=['commit', 'date', 'version', 'suite', 'benchmark', 'experiment', 'name', 'config', 'case',], as_index=False)\n",
    "                .median()\n",
    "                .drop(['commit', 'date', 'version', 'suite', 'benchmark', 'case', 'runid'], axis=1)\n",
    "           )\n",
    "    experiment = data['experiment'].unique()\n",
    "    assert len(experiment) == 1, f'experiment contains multiple different queries'\n",
    "    assert experiment[0] == query, f'experiment query: {experiment[0]} does not match query {query} of current file'\n",
    "\n",
    "    assert data.shape[0] == 1, f'after aggregating, dataframe should only contain one row'\n",
    "    \n",
    "    post_join_times.append(data.iloc[0]['time'])\n",
    "    \n",
    "# plot data\n",
    "x = np.arange(len(processed_queries))  # the label locations\n",
    "width = 0.30  # the width of the bars\n",
    "fig, ax = plt.subplots(figsize=set_size(fraction_width=1, fraction_height=0.20), layout='constrained')\n",
    "\n",
    "ax.bar(x, post_join_times, color=colormap['Single Table'], label='Single Table')\n",
    "\n",
    "ax.set_ylabel('Query execution time [ms]')\n",
    "ax.set_title('Post-join in mutable')\n",
    "ax.set_xlabel('JOB queries')\n",
    "ax.set_xticks(x, queries)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylim(0)\n",
    "ax.xaxis.grid(False) # Disable grid lines on the x-axis\n",
    "\n",
    "fig.savefig('job-postjoin-mutable.pdf', bbox_inches='tight')"
   ],
   "id": "97b8e1bb5114b6c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('../post-join/job/postjoin-postgres-results.csv')\n",
    "data = data.groupby(by=['database', 'system', 'method', 'query'], as_index=False).median().drop(['database', 'system', 'method', 'run'], axis=1)\n",
    "data = data.sort_values(by=['query'], key=natsort_keygen())\n",
    "\n",
    "queries = data['query']\n",
    "x = np.arange(len(queries))\n",
    "postjoin_times = data['time']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=set_size(fraction_width=1, fraction_height=0.20), layout='constrained')\n",
    "ax.bar(x, postjoin_times, color=colormap['Single Table'], label='Single Table')\n",
    "\n",
    "ax.set_ylabel('Query execution time [ms]')\n",
    "ax.set_title('Post-join in PostgreSQL')\n",
    "ax.set_xlabel('JOB queries')\n",
    "ax.set_xticks(x, queries)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylim(0)\n",
    "ax.xaxis.grid(False) # Disable grid lines on the x-axis\n",
    "\n",
    "fig.savefig('job-postjoin-postgres.pdf', bbox_inches='tight')"
   ],
   "id": "817823890c499b47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# End-to-End Runtime Including Query Execution, Data Transfer, & Post-join",
   "id": "5b5c3d8c39bad9fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Rewrite Methods",
   "id": "18f855562bf8c4d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Join-Order Benchmark",
   "id": "e58fa0d2c62b5a6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def bar_plot(data, filename):\n",
    "    queries = data['query']\n",
    "    x = np.arange(len(queries))  # the label locations\n",
    "    width = 0.20  # the width of the bars\n",
    "    fig, ax = plt.subplots(figsize=set_size(fraction_width=1, fraction_height=0.2), layout='constrained')\n",
    "\n",
    "    # Single Table\n",
    "    ax.bar(x, data['st_exec_time'], width, color=colormap['Single Table'], edgecolor='black',label='Single Table')\n",
    "    ax.bar(x, data['st_transfer_time'],  width=width, bottom=data['st_exec_time'], color=colormap['ST transfer time'], edgecolor='black', label='Single Table Transfer Time')\n",
    "\n",
    "    # Rewrite Method\n",
    "    ax.bar(x + width, data['rm_exec_time'], width=width, color=colormap['Result DB'], edgecolor='black', label='Result DB')\n",
    "    ax.bar(x + width, data['rm_transfer_time'], width=width, bottom=data['rm_exec_time'], color=colormap['RDB transfer time'], edgecolor='black', label='Result DB Transfer Time')\n",
    "    ax.bar(x + width, data['rm_post_join_time'], width=width, bottom=data['rm_exec_time'] + data['rm_transfer_time'], color=colormap['RDB post-join'], edgecolor='black', label='Result DB Post-join')\n",
    "\n",
    "    ax.set_xticks(x + width / 2, queries)\n",
    "    ax.set_xlabel('JOB queries')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.xaxis.grid(False)  # Disable grid lines on the x-axis\n",
    "\n",
    "    log_scale = False\n",
    "    if log_scale:\n",
    "        ax.set_ylabel('Runtime [ms] (log-scale)')\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.set_ylabel('Runtime [ms]')\n",
    "        ax.set_ylim(0)\n",
    "\n",
    "    fig.savefig(filename, bbox_inches='tight')\n",
    "\n",
    "def to_latex(data, entries_per_row):\n",
    "    table_top = f\"\\\\begin{{table*}}\\n\"\n",
    "    table_top += f\"\\t\\\\small\\n\"\n",
    "    table_top += f\"\\t\\\\caption{{End-to-end performance of the best rewrite method~(RM) compared to the Single Table (ST) execution on JOB.}} \\n\"\n",
    "    table_top += f\"\\t\\\\label{{tab:job-end-to-end-rewrite-methods}}\\n\"\n",
    "\n",
    "    tabular = \"\"\n",
    "    queries = natsorted(data['query'].unique())\n",
    "    num_queries = len(queries)\n",
    "    assert num_queries % entries_per_row == 0, \"number of queries should be a multiple of entries per row\"\n",
    "    num_tabulars = num_queries // entries_per_row\n",
    "    for t in range(num_tabulars):\n",
    "        tabular_queries = queries[t * entries_per_row:t * entries_per_row + entries_per_row]\n",
    "        tabular += f\"\\t\\\\begin{{tabular}}{{\\n\"\n",
    "        tabular += f\"\\t\\t|l!{{\\\\vrule width 1.5pt}}\\n\"\n",
    "        for e in range(entries_per_row):\n",
    "            if e != entries_per_row - 1:\n",
    "                tabular += f\"\\t\\tR{{\\\\cellwidth}}|R{{\\\\cellwidth}}!{{\\\\vrule width \\\\vw}}\\n\"\n",
    "            else:\n",
    "                tabular += f\"\\t\\tR{{\\\\cellwidth}}|R{{\\\\cellwidth}}|}}\\n\"\n",
    "        tabular += f\"\\t\\t\\\\hline\\n\"\n",
    "        tabular += f\"\\t\\t\\\\textbf{{JOB Query}}\\n\"\n",
    "        for i, q in enumerate(tabular_queries):\n",
    "            if i != entries_per_row - 1:\n",
    "                tabular += f\"\\t\\t& \\\\multicolumn{{2}}{{c!{{\\\\vrule width \\\\vw}}}}{{\\\\textbf{{{q[1:]}}}}}\\n\"\n",
    "            else:\n",
    "                tabular += f\"\\t\\t& \\\\multicolumn{{2}}{{c|}}{{\\\\textbf{{{q[1:]}}}}}\\n\"\n",
    "        tabular += f\"\\t\\t\\\\\\\\\\\\hline\\n\"\n",
    "        tabular += f\"\\t\\t\\\\textbf{{Approach}}\"\n",
    "        for e in range(entries_per_row):\n",
    "            if e != entries_per_row - 1:\n",
    "                tabular += f\"\\t\\t& \\\\multicolumn{{1}}{{c|}}{{\\\\textbf{{ST}}}} & \\\\multicolumn{{1}}{{c!{{\\\\vrule width \\\\vw}}}}{{\\\\textbf{{RM}}}}\\n\"\n",
    "            else:\n",
    "                tabular += f\"\\t\\t& \\\\multicolumn{{1}}{{c|}}{{\\\\textbf{{ST}}}} & \\\\multicolumn{{1}}{{c|}}{{\\\\textbf{{RM}}}}\\n\"\n",
    "        tabular += f\"\\t\\t\\\\\\\\ \\\\noalign{{\\\\hrule height 1.5pt}}\\n\"\n",
    "        query_execution_time = \"\\t\\tQuery Execution [ms]\"\n",
    "        data_transfer_time = \"\\t\\tData Transfer [ms]\"\n",
    "        post_join_time = \"\\t\\tPost-join [ms]\"\n",
    "        sum_time = \"\\t\\t$\\\\sum$ [ms]\"\n",
    "        for q in tabular_queries:\n",
    "            qet_st = data[data['query'] == q]['st_exec_time'].values[0]\n",
    "            qet_rm = data[data['query'] == q]['rm_exec_time'].values[0]\n",
    "            dt_st = data[data['query'] == q]['st_transfer_time'].values[0]\n",
    "            dt_rm = data[data['query'] == q]['rm_transfer_time'].values[0]\n",
    "            pj_rm = data[data['query'] == q]['rm_post_join_time'].values[0]\n",
    "            query_execution_time += f\" & {round(qet_st, 2)} & {round(qet_rm, 2)}\"\n",
    "            data_transfer_time += f\" & {round(dt_st, 2)} & {round(dt_rm, 2)}\"\n",
    "            post_join_time += f\" & - & {round(pj_rm, 2)}\"\n",
    "            sum_time += f\" & {round(qet_st + dt_st, 2)} & {round(qet_rm + dt_rm + pj_rm, 2)}\"\n",
    "        tabular += f\"{query_execution_time}\\\\\\\\\\n\"\n",
    "        tabular += f\"{data_transfer_time}\\\\\\\\\\n\"\n",
    "        tabular += f\"{post_join_time}\\\\\\\\\\\\hline\\n\"\n",
    "        tabular += f\"{sum_time}\\\\\\\\\\\\hline\\n\"\n",
    "        tabular += f\"\\t\\\\end{{tabular}}\\n\"\n",
    "    table_bottom = f\"\\\\end{{table*}}\\n\"\n",
    "    latex = f\"{table_top}{tabular}{table_bottom}\"\n",
    "    print(latex)\n",
    "\n",
    "queries = job_subset\n",
    "\n",
    "data = pd.read_csv('../rewrite-methods/job/rewrite-results.csv')\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer', 'run'], as_index=False).sum()\n",
    "data = data.groupby(by=['database', 'system', 'query', 'method', 'data_transfer'], as_index=False).median().drop(\n",
    "    ['run', 'num_query_internal'], axis=1)\n",
    "data = data.drop(['database', 'system'], axis=1)\n",
    "data  = data[data['query'].isin(queries)]\n",
    "# Remove measurements with data transfer\n",
    "data = data[data['data_transfer'] == False]\n",
    "\n",
    "# Extract Single Table query execution times\n",
    "single_table_execution_time = data[data['method'] == '0. Single Table'].sort_values(by=['query'], key=natsort_keygen())['time'].array\n",
    "\n",
    "# Compute best RM\n",
    "data_rm = pd.read_csv('../rewrite-methods/job/rewrite-results-post-join.csv')\n",
    "data_rm = data_rm.groupby(by=['database', 'system', 'query', 'method', 'data_transfer', 'run'], as_index=False).sum()\n",
    "data_rm = data_rm.groupby(by=['database', 'system', 'query', 'method', 'data_transfer'], as_index=False).median().drop(\n",
    "    ['run', 'num_query_internal'], axis=1)\n",
    "data_rm = data_rm.drop(['database', 'system'], axis=1)\n",
    "# Remove measurements with data transfer\n",
    "data_rm = data_rm[data_rm['data_transfer'] == False]\n",
    "data_rm = data_rm[data_rm['method'] != '0. Single Table']\n",
    "minimal_idx = data_rm.groupby(by=['query'])['time'].idxmin()\n",
    "data_rm = data_rm.loc[minimal_idx]\n",
    "data_rm = data_rm.sort_values(by=['query'], key=natsort_keygen())\n",
    "best_rm_execution_time = data_rm['time'].array\n",
    "\n",
    "# Compute result set size for single table and result DB\n",
    "result_sizes_df = pd.read_csv('../result-set-sizes/job/result-set-sizes.csv')\n",
    "result_sizes_df = result_sizes_df[result_sizes_df['query'].isin(queries)]\n",
    "result_sizes_df = result_sizes_df.drop(['relation', 'count'], axis=1)\n",
    "result_sizes_df = result_sizes_df.groupby(by=['database', 'query', 'method'], as_index=False).sum()\n",
    "result_sizes_df = result_sizes_df.sort_values(by=['query', 'method'], key=natsort_keygen())\n",
    "result_sizes_df['size'] = round(result_sizes_df['size'] / KB, 2)\n",
    "\n",
    "st_result_sizes = result_sizes_df[result_sizes_df['method'] == 'Single Table']['size'].array\n",
    "rm_result_sizes = result_sizes_df[result_sizes_df['method'] == 'rdb_w_post_join_info']['size'].array\n",
    "\n",
    "# Compute the data transfer time for a given data transfer rate\n",
    "data_transfer_rate = 100 # Mbps\n",
    "st_transfer_times = [(((size / 1024) * 8) / data_transfer_rate) * 1000 for size in st_result_sizes]\n",
    "rm_transfer_times = [(((size / 1024) * 8) / data_transfer_rate) * 1000 for size in rm_result_sizes]\n",
    "\n",
    "# Extract post-join times\n",
    "post_join_times_df = pd.read_csv('../post-join/job/postjoin-postgres-results.csv')\n",
    "post_join_times_df = post_join_times_df[post_join_times_df['query'].isin(queries)]\n",
    "post_join_times_df = post_join_times_df.groupby(by=['database', 'system', 'method', 'query'], as_index=False).median().drop(['database', 'system', 'method', 'run'], axis=1)\n",
    "post_join_times_df = post_join_times_df.sort_values(by=['query'], key=natsort_keygen())\n",
    "post_join_times = post_join_times_df['time'].array\n",
    "\n",
    "# Construct final dataframe \n",
    "data = pd.DataFrame(data= {\n",
    "    'query': data_rm['query'],\n",
    "    'st_exec_time': single_table_execution_time,\n",
    "    'rm_exec_time': best_rm_execution_time,\n",
    "    'st_transfer_time': st_transfer_times,\n",
    "    'rm_transfer_time': rm_transfer_times,\n",
    "    'rm_post_join_time': post_join_times,\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "to_latex(data, 5)"
   ],
   "id": "35a28de5418a92c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Algorithm",
   "id": "aa16152c080fd366"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Join-Order Benchmark",
   "id": "cffac94418429de0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def bar_plot(data, filename):\n",
    "    queries = data['query']\n",
    "    x = np.arange(len(queries))  # the label locations\n",
    "    width = 0.20  # the width of the bars\n",
    "    fig, ax = plt.subplots(figsize=set_size(fraction_width=1, fraction_height=0.2), layout='constrained')\n",
    "    \n",
    "    # Single Table\n",
    "    ax.bar(x, data['st_exec_time'], width, color=colormap['Single Table'], edgecolor='black',label='Single Table')\n",
    "    ax.bar(x, data['st_transfer_time'],  width=width, bottom=data['st_exec_time'], color=colormap['ST transfer time'], edgecolor='black', label='Single Table Transfer Time')\n",
    "    \n",
    "    # Rewrite Method\n",
    "    ax.bar(x + width, data['rdb_exec_time'], width=width, color=colormap['Result DB'], edgecolor='black', label='Result DB')\n",
    "    ax.bar(x + width, data['rdb_transfer_time'], width=width, bottom=data['rdb_exec_time'], color=colormap['RDB transfer time'], edgecolor='black', label='Result DB Transfer Time')\n",
    "    ax.bar(x + width, data['rdb_post_join_time'], width=width, bottom=data['rdb_exec_time'] + data['rdb_transfer_time'], color=colormap['RDB post-join'], edgecolor='black', label='Result DB Post-join')\n",
    "    \n",
    "    ax.set_xticks(x + width / 2, queries)\n",
    "    ax.set_xlabel('JOB queries')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.xaxis.grid(False)  # Disable grid lines on the x-axis\n",
    "\n",
    "    log_scale = False\n",
    "    if log_scale:\n",
    "        ax.set_ylabel('Runtime [ms] (log-scale)')\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.set_ylabel('Runtime [ms]')\n",
    "        ax.set_ylim(0)\n",
    "\n",
    "    fig.savefig(filename, bbox_inches='tight')\n",
    "\n",
    "\n",
    "def to_latex(data, entries_per_row):\n",
    "    table_top = f\"\\\\begin{{table*}}\\n\"\n",
    "    table_top += f\"\\t\\\\small\\n\"\n",
    "    table_top += f\"\\t\\\\caption{{End-to-end performance of the \\\\textsc{{ResultDB}}$_{{\\\\textsc{{semi-join}}}}$ algorithm compared to the Single Table (ST) execution on JOB.}} \\n\"\n",
    "    table_top += f\"\\t\\\\label{{tab:job-end-to-end-result-db}}\\n\"\n",
    "\n",
    "    tabular = \"\"\n",
    "    queries = natsorted(data['query'].unique())\n",
    "    num_queries = len(queries)\n",
    "    assert num_queries % entries_per_row == 0, \"number of queries should be a multiple of entries per row\"\n",
    "    num_tabulars = num_queries // entries_per_row\n",
    "    for t in range(num_tabulars):\n",
    "        tabular_queries = queries[t * entries_per_row:t * entries_per_row + entries_per_row]\n",
    "        tabular += f\"\\t\\\\begin{{tabular}}{{\\n\"\n",
    "        tabular += f\"\\t\\t|l!{{\\\\vrule width 1.5pt}}\\n\"\n",
    "        for e in range(entries_per_row):\n",
    "            if e != entries_per_row - 1:\n",
    "                tabular += f\"\\t\\tR{{\\\\cellwidth}}|R{{\\\\cellwidth}}!{{\\\\vrule width \\\\vw}}\\n\"\n",
    "            else:\n",
    "                tabular += f\"\\t\\tR{{\\\\cellwidth}}|R{{\\\\cellwidth}}|}}\\n\"\n",
    "        tabular += f\"\\t\\t\\\\hline\\n\"\n",
    "        tabular += f\"\\t\\t\\\\textbf{{JOB Query}}\\n\"\n",
    "        for i, q in enumerate(tabular_queries):\n",
    "            if i != entries_per_row - 1:\n",
    "                tabular += f\"\\t\\t& \\\\multicolumn{{2}}{{c!{{\\\\vrule width \\\\vw}}}}{{\\\\textbf{{{q[1:]}}}}}\\n\"\n",
    "            else:\n",
    "                tabular += f\"\\t\\t& \\\\multicolumn{{2}}{{c|}}{{\\\\textbf{{{q[1:]}}}}}\\n\"\n",
    "        tabular += f\"\\t\\t\\\\\\\\\\\\hline\\n\"\n",
    "        tabular += f\"\\t\\t\\\\textbf{{Approach}}\"\n",
    "        for e in range(entries_per_row):\n",
    "            if e != entries_per_row - 1:\n",
    "                tabular += f\"\\t\\t& \\\\multicolumn{{1}}{{c|}}{{\\\\textbf{{ST}}}} & \\\\multicolumn{{1}}{{c!{{\\\\vrule width \\\\vw}}}}{{\\\\textbf{{RDB$_{{\\\\text{{RP}}}}$}}}}\\n\"\n",
    "            else:\n",
    "                tabular += f\"\\t\\t& \\\\multicolumn{{1}}{{c|}}{{\\\\textbf{{ST}}}} & \\\\multicolumn{{1}}{{c|}}{{\\\\textbf{{RDB$_{{\\\\text{{RP}}}}$}}}}\\n\"\n",
    "        tabular += f\"\\t\\t\\\\\\\\ \\\\noalign{{\\\\hrule height 1.5pt}}\\n\"\n",
    "        query_execution_time = \"\\t\\tQuery Execution [ms]\"\n",
    "        data_transfer_time = \"\\t\\tData Transfer [ms]\"\n",
    "        post_join_time = \"\\t\\tPost-join [ms]\"\n",
    "        sum_time = \"\\t\\t$\\\\sum$ [ms]\"\n",
    "        for q in tabular_queries:\n",
    "            qet_st = data[data['query'] == q]['st_exec_time'].values[0]\n",
    "            qet_rdb = data[data['query'] == q]['rdb_exec_time'].values[0]\n",
    "            dt_st = data[data['query'] == q]['st_transfer_time'].values[0]\n",
    "            dt_rdb = data[data['query'] == q]['rdb_transfer_time'].values[0]\n",
    "            pj_rdb = data[data['query'] == q]['rdb_post_join_time'].values[0]\n",
    "            query_execution_time += f\" & {round(qet_st, 2)} & {round(qet_rdb, 2)}\"\n",
    "            data_transfer_time += f\" & {round(dt_st, 2)} & {round(dt_rdb, 2)}\"\n",
    "            post_join_time += f\" & - & {round(pj_rdb, 2)}\"\n",
    "            sum_time += f\" & {round(qet_st + dt_st, 2)} & {round(qet_rdb + dt_rdb + pj_rdb, 2)}\"\n",
    "        tabular += f\"{query_execution_time}\\\\\\\\\\n\"\n",
    "        tabular += f\"{data_transfer_time}\\\\\\\\\\n\"\n",
    "        tabular += f\"{post_join_time}\\\\\\\\\\\\hline\\n\"\n",
    "        tabular += f\"{sum_time}\\\\\\\\\\\\hline\\n\"\n",
    "        tabular += f\"\\t\\\\end{{tabular}}\\n\"\n",
    "    table_bottom = f\"\\\\end{{table*}}\\n\"\n",
    "    latex = f\"{table_top}{tabular}{table_bottom}\"\n",
    "    print(latex)\n",
    "\n",
    "# Extract Single Table and Result DB execution times in mutable\n",
    "# ensure that `queries` is sorted!\n",
    "queries = [q for q in job_subset if q != \"q16b\"]\n",
    "\n",
    "st_exec_times = []\n",
    "rdb_exec_times = []\n",
    "processed_queries = []\n",
    "num_queries = 0\n",
    "# TODO: update the mutable results! currently, we compute RDB and not RDB_RP, i.e., we do not have a relationship-preserving database\n",
    "for query in queries:\n",
    "    result_file = Path(f\"../algorithm/job/{query}_results.csv\")\n",
    "    if not result_file.is_file():\n",
    "        print(f\"{result_file} does not exist. Skipping...\")\n",
    "        continue\n",
    "    processed_queries.append(query)\n",
    "    data = pd.read_csv(result_file)  \n",
    "    data = (data.groupby(by=['commit', 'date', 'version', 'suite', 'benchmark', 'experiment', 'name', 'config', 'case',], as_index=False)\n",
    "                .median()\n",
    "                .drop(['commit', 'date', 'version', 'suite', 'benchmark', 'case', 'runid'], axis=1)\n",
    "           )\n",
    "    experiment = data['experiment'].unique()\n",
    "    assert len(experiment) == 1, f'experiment contains multiple different queries'\n",
    "    assert experiment[0] == query, f'experiment query: {experiment[0]} does not match query {query} of current file'\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        algorithm = row['name']\n",
    "        execution_time = row['time']\n",
    "        if \"single-table\" in algorithm:\n",
    "            single_table_execution_time = execution_time\n",
    "        elif \"decompose\" in algorithm:\n",
    "            pass\n",
    "        elif \"resultdb\" in algorithm:\n",
    "            resultdb_execution_time = execution_time\n",
    "        else:\n",
    "            assert False, \"experiment name: {name} does not match any of our algorithms\"\n",
    "\n",
    "    st_exec_times.append(single_table_execution_time)\n",
    "    rdb_exec_times.append(resultdb_execution_time)\n",
    "\n",
    "assert len(st_exec_times) == len(rdb_exec_times) == len(processed_queries), f'number of measurements has to match the processed queries'\n",
    "\n",
    "# Compute result set size for single table and ResultDB\n",
    "result_sizes_df = pd.read_csv('../result-set-sizes/job/result-set-sizes.csv')\n",
    "result_sizes_df = result_sizes_df.drop(['relation', 'count'], axis=1)\n",
    "result_sizes_df = result_sizes_df.groupby(by=['database', 'query', 'method'], as_index=False).sum()\n",
    "result_sizes_df = result_sizes_df.sort_values(by=['query', 'method'], key=natsort_keygen())\n",
    "result_sizes_df = result_sizes_df[result_sizes_df['query'].isin(queries)]\n",
    "result_sizes_df['size'] = round(result_sizes_df['size'] / KB, 2)\n",
    "\n",
    "st_result_sizes = result_sizes_df[result_sizes_df['method'] == 'Single Table']['size'].array\n",
    "rdb_result_sizes = result_sizes_df[result_sizes_df['method'] == 'rdb_w_post_join_info']['size'].array\n",
    "\n",
    "# Compute the data transfer time for a given data transfer rate\n",
    "data_transfer_rate = 100 # Mbps\n",
    "st_transfer_times = [(((size / 1024) * 8) / data_transfer_rate) * 1000 for size in st_result_sizes]\n",
    "rdb_transfer_times = [(((size / 1024) * 8) / data_transfer_rate) * 1000 for size in rdb_result_sizes]\n",
    "\n",
    "# Extract post-join times\n",
    "post_join_times = []\n",
    "processed_queries = []\n",
    "num_queries = 0\n",
    "for query in queries:\n",
    "    result_file = Path(f\"../post-join/job/{query}/{query}_results.csv\")\n",
    "    if not result_file.is_file():\n",
    "        print(f\"{result_file} does not exist. Skipping...\")\n",
    "        continue\n",
    "    processed_queries.append(query)\n",
    "    data = pd.read_csv(result_file)  \n",
    "    data = (data.groupby(by=['commit', 'date', 'version', 'suite', 'benchmark', 'experiment', 'name', 'config', 'case',], as_index=False)\n",
    "                .median()\n",
    "                .drop(['commit', 'date', 'version', 'suite', 'benchmark', 'case', 'runid'], axis=1)\n",
    "           )\n",
    "    experiment = data['experiment'].unique()\n",
    "    assert len(experiment) == 1, f'experiment contains multiple different queries'\n",
    "    assert experiment[0] == query, f'experiment query: {experiment[0]} does not match query {query} of current file'\n",
    "\n",
    "    assert data.shape[0] == 1, f'after aggregating, dataframe should only contain one row'\n",
    "    \n",
    "    post_join_times.append(data.iloc[0]['time'])\n",
    "\n",
    "# Construct final dataframe \n",
    "data = pd.DataFrame(data= {\n",
    "    'query': queries,\n",
    "    'st_exec_time': st_exec_times,\n",
    "    'rdb_exec_time': rdb_exec_times,\n",
    "    'st_transfer_time': st_transfer_times,\n",
    "    'rdb_transfer_time': rdb_transfer_times,\n",
    "    'rdb_post_join_time': post_join_times,\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "to_latex(data, 3)"
   ],
   "id": "1d085e6aeba03986",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
